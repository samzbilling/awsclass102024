+ Let's go through the entire process, step by step, from scratch to create the solution that involves AWS Lambda, S3, DynamoDB, SNS, SES, CloudWatch, Athena, and QuickSight. We'll cover the setup of all AWS resources and then walk through the Lambda function code and testing process.

Part 1: Automate File Processing with Lambda
Step 1: Set Up AWS Resources
1.1. Create an S3 Bucket
Go to the S3 console and create a new bucket.

Name the bucket (e.g., my-file-processing-bucket).
Choose your preferred region.
Leave all other settings at their default values.
Set up event notifications for the bucket to trigger the Lambda function whenever a file is uploaded:

Go to the Properties tab of the bucket.
Scroll down to Event notifications and click Create event notification.
Give the event a name (e.g., FileUploadEvent).
Under Event types, select All object create events.
Under Send to, choose Lambda function.
Select the Lambda function you'll create later (we'll do this after creating the Lambda function).
1.2. Create DynamoDB Table
Go to the DynamoDB console.
Click on Create table.
Table name: FileMetadataTable
Primary Key: Set the Partition key to FileName (String).
Additional attributes:
BucketName (String)
ContentType (String)
FileSize (Number)
LastModified (String)
Click Create.
1.3. Create SNS Topic for Notifications
Go to the SNS console.
Click on Create topic.
Type: Choose Standard.
Name: FileUploadNotifications
Click Create topic.
Add subscribers to the topic (e.g., email).
Click on Create subscription.
Protocol: Choose Email.
Endpoint: Enter your email address.
Confirm the subscription by checking your email and clicking the confirmation link.
1.4. Set Up SES for Email Reports
Go to the SES console.
Verify email addresses:
Navigate to Email Addresses under Identity Management.
Add and verify both your sender and recipient email addresses (required for sandbox mode).
Configure SES for sending emails in the Lambda function (we'll do this in the code).
1.5. Create an IAM Role for Lambda Function
Go to the IAM console.
Click Create role.
Select AWS service as the trusted entity and choose Lambda.
Attach the following policies:
AmazonS3ReadOnlyAccess
AmazonDynamoDBFullAccess
AmazonSNSFullAccess
AmazonSESFullAccess
CloudWatchLogsFullAccess
Give the role a name (e.g., LambdaFileProcessingRole).
Click Create role.
Step 2: Create and Configure the Lambda Function
2.1. Create the Lambda Function
Go to the Lambda console.
Click Create function.
Name: FileProcessorFunction
Runtime: Choose Python 3.x.
Role: Choose the IAM role you created earlier (LambdaFileProcessingRole).
Click Create function.
2.2. Write the Lambda Function Code
Here is the complete Lambda function code to handle file uploads, store metadata in DynamoDB, send SNS notifications, send SES emails, and log events to CloudWatch:

python
Copy code
import json
import boto3
import logging
import time

# Initialize AWS services
s3_client = boto3.client('s3')
dynamodb_client = boto3.client('dynamodb')
sns_client = boto3.client('sns')
ses_client = boto3.client('ses')
cloudwatch_client = boto3.client('logs')

# Log setup
logging.basicConfig(level=logging.INFO)

def lambda_handler(event, context):
    try:
        # Extract event details
        bucket_name = event['Records'][0]['s3']['bucket']['name']
        file_name = event['Records'][0]['s3']['object']['key']
        
        # Step 2.2: Retrieve file metadata using head_object
        file_metadata = s3_client.head_object(Bucket=bucket_name, Key=file_name)
        
        # Extract relevant metadata
        content_type = file_metadata['ContentType']
        file_size = file_metadata['ContentLength']
        last_modified = file_metadata['LastModified'].strftime("%Y-%m-%d %H:%M:%S")
        
        # Step 2.3: Store metadata in DynamoDB
        dynamodb_client.put_item(
            TableName='FileMetadataTable',
            Item={
                'FileName': {'S': file_name},
                'BucketName': {'S': bucket_name},
                'ContentType': {'S': content_type},
                'FileSize': {'N': str(file_size)},
                'LastModified': {'S': last_modified}
            }
        )
        
        # Step 2.4: Publish SNS notification
        sns_message = f"New file uploaded: {file_name} in bucket {bucket_name}. Size: {file_size} bytes."
        sns_client.publish(
            TopicArn='arn:aws:sns:REGION:ACCOUNT_ID:FileUploadNotifications',
            Message=sns_message,
            Subject="File Upload Notification"
        )
        
        # Step 2.5: Send email report via SES
        email_subject = f"New File Uploaded: {file_name}"
        email_body = f"""
        A new file has been uploaded:
        File Name: {file_name}
        Bucket Name: {bucket_name}
        Content Type: {content_type}
        File Size: {file_size} bytes
        Last Modified: {last_modified}
        """
        ses_client.send_email(
            Source='sender@example.com',
            Destination={
                'ToAddresses': ['recipient@example.com']
            },
            Message={
                'Subject': {'Data': email_subject},
                'Body': {'Text': {'Data': email_body}}
            }
        )
        
        # Step 2.6: Log the event in CloudWatch
        logging.info(f"Successfully processed file: {file_name}")
        cloudwatch_client.put_log_events(
            logGroupName='FileProcessingLogGroup',
            logStreamName='FileProcessingStream',
            logEvents=[
                {
                    'timestamp': int(time.time() * 1000),
                    'message': f"Processed file {file_name} from bucket {bucket_name}"
                }
            ]
        )

        return {
            'statusCode': 200,
            'body': json.dumps('File processed successfully')
        }

    except Exception as e:
        logging.error(f"Error processing file: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps('Error processing file')
        }
Explanation:
The function is triggered by an S3 event (when a file is uploaded).
It retrieves file metadata using head_object and stores this data in DynamoDB.
It sends a notification via SNS and a detailed email via SES.
It logs events to CloudWatch for monitoring.

2.3. Add the Trigger for Lambda
Go to the Lambda function you created.
Under Configuration, go to Triggers and click Add Trigger.
Select S3, choose the bucket (my-file-processing-bucket), and configure it to trigger on PUT events (file uploads).
Step 3: Deploy and Test the Lambda Function
Deploy the Lambda function: Click Deploy in the Lambda console.
Upload a file to the S3 bucket (my-file-processing-bucket).

Verify the following:
The file metadata is stored in the DynamoDB table (FileMetadataTable).
An SNS notification is sent to the subscriber.
A SES email is sent with file details.
Check CloudWatch for logs (you should see the log entry confirming the file was processed).

## Part 2: Query Data with Athena

Step 4: Set Up Athena for Querying

Create an S3 Data Catalog:

Go to the Athena console.
Create a database (e.g., file_metadata_db).
Create a table that points to your S3 bucket (my-file-processing-bucket) and define the schema based on the metadata structure.

Sample Athena Query:

SELECT BucketName, COUNT(FileName) AS FileCount
FROM file_metadata_db.FileMetadataTable
GROUP BY BucketName;


Part 3: Visualize Data in QuickSight

Step 5: Set Up QuickSight for Visualizations

Connect QuickSight to Athena:

Go to the QuickSight console.
Create a new data source linked to Athena.
Import the dataset from the Athena query.
Create a Dashboard:

Create visualizations (e.g., bar charts) to represent the data.
Conclusion
You have successfully set up a complete end-to-end solution for file processing automation using AWS Lambda, S3, DynamoDB, SNS, SES, CloudWatch, Athena, and QuickSight. You can now upload files to S3, have metadata stored in DynamoDB, receive notifications, and create reports.



