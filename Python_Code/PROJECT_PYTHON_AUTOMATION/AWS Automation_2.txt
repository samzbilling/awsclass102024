
Let's walk through the entire process, step-by-step, to create an automated file processing solution using AWS services like Lambda, S3, DynamoDB, SNS, SES, CloudWatch, Athena, and QuickSight. This solution will automatically handle file uploads, store metadata, send notifications, and enable querying and visualization of the data. We'll cover how to set up all the necessary AWS resources, write the Lambda function code, and test the solution.
Part 1: Automate File Processing with Lambda
Step 1: Set Up AWS Resources
1.1. Create an S3 Bucket
Go to the Amazon S3 console.
Click Create bucket.
Bucket Name: Choose a unique name for your bucket (e.g., my-file-processing-bucket).
Region: Choose the AWS region where you want the bucket to reside.
Leave all other settings as default for now, or customize as necessary.
Once the bucket is created, configure it to trigger a Lambda function when a file is uploaded.
Go to the Properties tab of the bucket.
Scroll down to Event notifications and click Create event notification.
Event name: Enter a descriptive name like FileUploadEvent.
Event types: Select All object create events to trigger on any file upload.
Send to: Choose Lambda function.
Lambda function: (Leave empty for now; we will select the Lambda function after it's created).
1.2. Create DynamoDB Table
Go to the Amazon DynamoDB console.
Click Create table.
Table Name: Enter FileMetadataTable.
Primary Key: Set Partition key to FileName (String) to uniquely identify each file.
Add additional attributes:
BucketName (String)
ContentType (String)
FileSize (Number)
LastModified (String)
Click Create to create the table.
1.3. Create SNS Topic for Notifications
Go to the Amazon SNS console.
Click Create topic.
Type: Choose Standard.
Name: Enter FileUploadNotifications.
After the topic is created, add a subscription:
Click Create subscription.
Protocol: Select Email.
Endpoint: Enter your email address.
Confirm the subscription by checking your email and clicking the confirmation link.
1.4. Set Up SES for Email Reports
Go to the Amazon SES console.
In Identity Management, go to Email Addresses and add both a Sender and Recipient email address.
You'll need to verify these email addresses if you're using SES in sandbox mode (this is required to send email).
Once the email addresses are verified, we will configure SES to send email reports from the Lambda function later.
1.5. Create an IAM Role for Lambda Function
Go to the IAM console.
Click Create role.
Trusted entity: Select AWS service and choose Lambda.
Attach the following permissions policies to the role:
AmazonS3ReadOnlyAccess (for read-only access to S3).
AmazonDynamoDBFullAccess (to read/write from DynamoDB).
AmazonSNSFullAccess (to send notifications via SNS).
AmazonSESFullAccess (to send emails using SES).
CloudWatchLogsFullAccess (for logging Lambda function activity).
Name the role LambdaFileProcessingRole and click Create role.
Step 2: Create and Configure the Lambda Function
2.1. Create the Lambda Function
Go to the Lambda console.
Click Create function.
Function name: Enter FileProcessorFunction.
Runtime: Choose Python 3.x.
Role: Select the IAM role you created earlier (LambdaFileProcessingRole).
Click Create function to complete the setup.
2.2. Write the Lambda Function Code
Here's the complete Lambda function code that handles the file upload process, including storing metadata in DynamoDB, sending SNS notifications, sending SES emails, and logging events to CloudWatch:

python
Copy code
import json
import boto3
import logging
import time

# Initialize AWS services
s3_client = boto3.client('s3')
dynamodb_client = boto3.client('dynamodb')
sns_client = boto3.client('sns')
ses_client = boto3.client('ses')
cloudwatch_client = boto3.client('logs')

# Log setup
logging.basicConfig(level=logging.INFO)

def lambda_handler(event, context):
    try:
        # Extract event details
        bucket_name = event['Records'][0]['s3']['bucket']['name']
        file_name = event['Records'][0]['s3']['object']['key']
        
        # Step 2.2: Retrieve file metadata using head_object
        file_metadata = s3_client.head_object(Bucket=bucket_name, Key=file_name)
        
        # Extract relevant metadata
        content_type = file_metadata['ContentType']
        file_size = file_metadata['ContentLength']
        last_modified = file_metadata['LastModified'].strftime("%Y-%m-%d %H:%M:%S")
        
        # Step 2.3: Store metadata in DynamoDB
        dynamodb_client.put_item(
            TableName='FileMetadataTable',
            Item={
                'FileName': {'S': file_name},
                'BucketName': {'S': bucket_name},
                'ContentType': {'S': content_type},
                'FileSize': {'N': str(file_size)},
                'LastModified': {'S': last_modified}
            }
        )
        
        # Step 2.4: Publish SNS notification
        sns_message = f"New file uploaded: {file_name} in bucket {bucket_name}. Size: {file_size} bytes."
        sns_client.publish(
            TopicArn='arn:aws:sns:REGION:ACCOUNT_ID:FileUploadNotifications',
            Message=sns_message,
            Subject="File Upload Notification"
        )
        
        # Step 2.5: Send email report via SES
        email_subject = f"New File Uploaded: {file_name}"
        email_body = f"""
        A new file has been uploaded:
        File Name: {file_name}
        Bucket Name: {bucket_name}
        Content Type: {content_type}
        File Size: {file_size} bytes
        Last Modified: {last_modified}
        """
        ses_client.send_email(
            Source='sender@example.com',
            Destination={
                'ToAddresses': ['recipient@example.com']
            },
            Message={
                'Subject': {'Data': email_subject},
                'Body': {'Text': {'Data': email_body}}
            }
        )
        
        # Step 2.6: Log the event in CloudWatch
        logging.info(f"Successfully processed file: {file_name}")
        cloudwatch_client.put_log_events(
            logGroupName='FileProcessingLogGroup',
            logStreamName='FileProcessingStream',
            logEvents=[{
                'timestamp': int(time.time() * 1000),
                'message': f"Processed file {file_name} from bucket {bucket_name}"
            }]
        )

        return {
            'statusCode': 200,
            'body': json.dumps('File processed successfully')
        }

    except Exception as e:
        logging.error(f"Error processing file: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps('Error processing file')
        }
Explanation of the Code:
The function is triggered by an S3 event (file upload).
It retrieves the file metadata (e.g., content type, size, and last modified date) using the head_object API from S3.
It stores the metadata in DynamoDB.
It sends an SNS notification with the file upload details.
It sends an email via SES with the same file details.
It logs the event to CloudWatch for monitoring purposes.
2.3. Add the Trigger for Lambda
Go to the Lambda function you created.
Under the Configuration tab, go to Triggers and click Add Trigger.
Select S3, choose your bucket (my-file-processing-bucket), and configure the trigger to fire on PUT events (i.e., when a file is uploaded).
Step 3: Deploy and Test the Lambda Function
Deploy the Lambda Function
Click Deploy in the Lambda console to deploy the function.
Upload a Test File to the S3 Bucket
Go to your S3 bucket (my-file-processing-bucket).
Upload a test file.
Verify the Following:
DynamoDB: The file metadata should be stored in the FileMetadataTable DynamoDB table.
SNS: You should receive an SNS notification about the uploaded file.
SES: An email should be sent with the file details.
CloudWatch Logs: Check the CloudWatch console to confirm that the event was logged.
Part 2: Query Data with Athena
Step 4: Set Up Athena for Querying
Create an S3 Data Catalog
Go to the Athena console.
Create a database (e.g., file_metadata_db).
Create a table that points to your S3 bucket (my-file-processing-bucket) and define the schema for the metadata stored in DynamoDB.
Sample Athena Query:

sql
Copy code
SELECT BucketName, COUNT(FileName) AS FileCount
FROM file_metadata_db.FileMetadataTable
GROUP BY BucketName;
Part 3: Visualize Data in QuickSight
Step 5: Set Up QuickSight for Visualizations
Go to the QuickSight console.
Create a new data source linked to Athena.
Import the dataset from Athena.
Create a dashboard with visualizations, such as bar charts or pie charts, to represent your data.
Conclusion
Congratulations! You've successfully set up a complete end-to-end solution for file processing automation using AWS Lambda, S3, DynamoDB, SNS, SES, CloudWatch, Athena, and QuickSight. You can now upload files to S3, automatically store their metadata in DynamoDB, receive notifications, and query and visualize the data!